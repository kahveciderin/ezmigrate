#!/usr/bin/python3

from multiprocessing import Process
import psycopg2
import pymongo
import json
import time
import copy
import sys


THREAD_COUNT_PER_OPERATION_PER_DATABASE = 1





db_settings = {}




def con_pql(dbname):
    try:
        return psycopg2.connect(user=db_settings["dbsetup"]["username"],
                                password=db_settings["dbsetup"]["password"],
                                host=db_settings["dbsetup"]["host"],
                                port=db_settings["dbsetup"]["port"],
                                database=dbname)
    except:
        print("Could not connect to the database {0}. Skipping...".format(dbname))
        return -1


def get_list_keyed_obj(obj, namespace):
    aaret = obj
    for name in namespace:
        aaret = aaret[name]
    return aaret

def glkol(obj, namespace, last, did = -1):
    if last.startswith('$$'):
        if last.startswith('$$lindex-'):
            indexNum = int(last.split('-')[1])
            theNum = 0
            for rhnum in reversed(namespace):
                if(type(rhnum) is int):
                    indexNum -= 1
                if indexNum < 0:
                    theNum = rhnum
                    break
            return theNum
        elif last.startswith('$$docn'):
            return did[0]
        elif last.startswith('$$psid'):
            return did[len(did) - 1]
        elif last.startswith('$$push-'):
            pushcol = int(last.split('-')[1])
            retval = []
            for a in glkol(obj, namespace, pushcol):
                retval.append(a)
            return retval
        if last.startswith('$$oids-'):
            id_hellstr = str(last.split('-')[1])
            return id_hellstr
        if last.startswith('$$oidc-'):
            id_hellstr = int(str(last.split('-')[1]), base=16)
            return id_hellstr
    else:
        newnsp = namespace + last.split('.')
        if newnsp[len(newnsp) - 1].startswith('#'): # access to parent element
            newnsp = newnsp[:-(int(newnsp[len(newnsp) - 1][1:]) + 1)]
        return get_list_keyed_obj(obj, copy.copy(newnsp))

def check_list_keyed_object(obj, namespace):
    aaret = obj
    for name in namespace:
        if type(name) is str and name.startswith('$$'):
            continue
        if type(aaret) is list:
            if len(aaret) <= name:
                return {}
        else: #dict
            if name not in aaret:
                return {}
        aaret = aaret[name]
    return aaret


def clkol(obj, namespace, last):
    if last.startswith('$$'):
        return True
    else:
        return last.split('.')[len(last.split('.')) - 1] in check_list_keyed_object(obj, copy.copy(namespace + last.split('.')[:-1]))

def recursive_eval(document_id, doc, wf, evals):
    pass

def recursive_exec(document_id, doc, operations, wf, pdb):
    pdb_name = copy.copy(pdb)
    pdb = con_pql(pdb) # immediately restore postgre database
    if pdb == -1:
        print('Fatal Error: Could not connect to the PostgreSQL database.')
        exit(-1) # will only exit the current process, effectively skipping the operation.
    for operation in operations:
        working_field = wf
        if(operation[0] == '$'): # this is a field name in the current document
            working_field.append(operation[1:])
            recursive_exec(document_id, doc, copy.copy(operations[operation]), copy.copy(working_field), pdb_name)
        elif(operation == "for"):
            for_index = 0
            for lot_for in get_list_keyed_obj(doc, working_field):
                new_wf = copy.copy(working_field)
                new_wf.append(copy.copy(for_index))
                for ops in operations[operation]:
                    recursive_exec(document_id, doc, copy.copy(ops), copy.copy(new_wf), pdb_name)
                for_index += 1
        elif(operation == "insert"):
            table_name = operations[operation]["name"]
            in_vals = {}
            ivl_vals = {}
            hiv_vals = ""
            for ivl in operations[operation]["fields"]:
                if clkol(doc, working_field, ivl):
                    indexed_ivl = operations[operation]["fields"][ivl]
                    in_vals[ivl] = glkol(doc, working_field, ivl, did=document_id)
                    ivl_vals[indexed_ivl] = glkol(doc, working_field, ivl, did=document_id) # i am too tired right now to understand what i did here
                hiv_vals += ivl + ','
            hiv_vals = hiv_vals[:-1]

            table_in_cols = ""
            table_val_cols = ""
            for tincol in in_vals:
                table_in_cols += operations[operation]["fields"][tincol] + ',' # sql injection
                table_val_cols += '%(' + operations[operation]["fields"][tincol] + ')s,' # sql injection
            table_in_cols = table_in_cols[:-1]
            table_val_cols = table_val_cols[:-1]

            postgres_cursor = pdb.cursor()
            

            sql_query = """INSERT INTO {0} ({1}) VALUES ({2}) RETURNING id""".format(table_name, table_in_cols, table_val_cols)
            if "options" in operations[operation]:
                if "unique" in operations[operation]["options"]:
                    sql_query = """INSERT INTO {0} ({1})
                    VALUES ({2}) ON CONFLICT ({4})
                    DO UPDATE SET {3}=EXCLUDED.{3} RETURNING id""".format(table_name, table_in_cols, table_val_cols, list(ivl_vals)[0], hiv_vals)
            # print('attempting: ', postgres_cursor.mogrify(sql_query, ivl_vals))
            postgres_cursor.execute(sql_query, ivl_vals)
            pdb.commit()
            apparent_id = postgres_cursor.fetchone()[0]
            if "after" in operations[operation]:
                aid = copy.copy(document_id)
                aid.append(apparent_id)
                recursive_exec(aid, doc, copy.copy(operations[operation]["after"]), copy.copy(working_field), pdb_name) # aid[0] is document id, aid[1] is id of the last inserted



if len(sys.argv) != 2:
    print("Usage: ezmigrate <database_schema.json>")
    exit(-1)

try:
    db_settings = json.loads(open(sys.argv[1]).read())
except Exception as e:
    print("Either the file "+ sys.argv[1] +" could not be found in your working directory, or it doesn't contain valid json.\nDetails:")
    print(e)
    exit(-1)
try:
    mclient = pymongo.MongoClient(db_settings["dbsetup"]["mongo_uri"])
except:
    print("Mongo server is not accessible")
    exit(-1)


THREAD_COUNT_PER_OPERATION_PER_DATABASE = db_settings["dbsetup"]["thread_count_per_operation_per_database"]
for database in db_settings["dbnames"]:
    print("Connecting to {0}...".format(database["output"]))
    postgre_db = database["output"] # we pass the name here because this will be reconnected every time while multiprocessing
    if(postgre_db == -1): continue
    print("Connecting to {0}...".format(database["input"]))
    mongo_db = mclient[database["input"]]

    for mtableop in db_settings["operations"]:
        mongo_table = mongo_db[mtableop] # first we get the table we want to work in
        document_id = 0
        thread_id = 0
        mongo_docs = mongo_table.find()
        jobs = []
        for doc in mongo_docs: # then we loop over all the documents
            while len(jobs) >= THREAD_COUNT_PER_OPERATION_PER_DATABASE:# thread count per database per operation
                for fjob in [job for job in jobs if job["thread"].is_alive()]:
                    print("Thread end", fjob["tid"])
                jobs = [job for job in jobs if job["thread"].is_alive()]
                # print("waiting for a job to finish... thread count: ", len(jobs))
                time.sleep(0.01)
            print("Thread start", thread_id)
            mongo_thread = Process(target=recursive_exec, args=([document_id], doc, copy.copy(db_settings["operations"][mtableop]), [], postgre_db))
            mongo_thread.start()
            jobs.append({"thread": mongo_thread, "tid": thread_id})
            document_id += 1
            thread_id += 1